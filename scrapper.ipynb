{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploration of Job Application Automation with Data Scraping\n",
    "### Introduction\n",
    "In today's competitive job market, efficiency is key. Automating repetitive tasks is one way to improve this efficiency. While full automation might not be possible due to varying application requirements and human interactions, partial automation can certainly help ease the burden. This article explores the technical aspects of automating job applications, focusing on the theoretical concept rather than practical application to ensure we abide by websites' terms of service.\n",
    "\n",
    "### What is Web Scraping?\n",
    "Web scraping is a technique for extracting information from websites. It automates the manual process of collecting data from the web, using various programming languages and tools.\n",
    "\n",
    "The first step in any web scraping project is to import the necessary Python packages and set up a web driver. The web driver allows us to programmatically control a web browser, enabling interactions with web pages just as a human user would.\n",
    "\n",
    "For example, we'll be using Selenium, a powerful tool for controlling a web browser through programs and automating browser automation. We'll also use pandas for data manipulation and time for pause delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "from selenium import webdriver # Web scraping library to control browser interaction\n",
    "import pandas as pd \n",
    "from time import sleep\n",
    "import requests\n",
    "from lxml import html\n",
    "import traceback\n",
    "\n",
    "# The following imports are included but not used in this code snippet. \n",
    "# They can be useful for advanced functionalities like waiting for certain conditions.\n",
    "# from selenium.webdriver.support.select import Select \n",
    "# from selenium.webdriver.support.ui import WebDriverWait \n",
    "# from selenium.webdriver.common.by import By \n",
    "# from selenium.webdriver.support import expected_conditions as EC \n",
    "\n",
    "# The import below is not used but can be important if you want to add Chrome options.\n",
    "# from selenium.webdriver.chrome.options import Options \n",
    "\n",
    "# Path to webdriver, can be chrome or anything else you have\n",
    "# Note: Replace the empty string with the path to your webdriver executable.\n",
    "driver = webdriver.Chrome(executable_path=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "driver = webdriver.Chrome(executable_path=\"\"): Initializes a new Chrome browser window controlled by the webdriver. Replace the empty string with the path to your ChromeDriver executable. Make sure you've downloaded the appropriate ChromeDriver version compatible with your Chrome browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating to LinkedIn and Logging In\n",
    "Once the necessary packages are imported and the webdriver is set up, the next step is to navigate to LinkedIn's login page. We will enter the username and password programmatically and then click the \"Sign In\" button.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for LinkedIn login\n",
    "url1='https://www.linkedin.com/login'\n",
    "\n",
    "# Implicitly wait for 1 second; gives the browser time to load scripts\n",
    "driver.implicitly_wait(1)\n",
    "\n",
    "# Navigate to the LinkedIn login page\n",
    "driver.get(url1)\n",
    "\n",
    "# Locate email field by its id and input email\n",
    "email_field = driver.find_element_by_id('username')\n",
    "email_field.send_keys('') # Replace empty quotes with your email\n",
    "print('- Finish keying in email')\n",
    "sleep(1) # Pause for 1 second\n",
    "\n",
    "# Locate password field by its name attribute and input password\n",
    "password_field = driver.find_element_by_name('session_password')\n",
    "password_field.send_keys('') # Replace empty quotes with your password\n",
    "print('- Finish keying in password')\n",
    "sleep(1) # Pause for 1 second\n",
    "\n",
    "# Locate the sign-in button by its XPath and click it\n",
    "signin_field = driver.find_element_by_xpath('//*[@id=\"organic-div\"]/form/div[3]/button')\n",
    "signin_field.click()\n",
    "sleep(1) # Pause for 1 second\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Company Names You Would Like to Apply To\n",
    "After successfully logging in, you might want to fetch a list of companies where you'd like to apply. For this example, we'll be scraping a list of hedge fund managers from the SWFI Institute website and saving the names to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page with a list of companies you're interested in\n",
    "url = \"https://www.swfinstitute.org/fund-manager-rankings/hedge-fund-manager\"\n",
    "\n",
    "# Fetch the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "tree = html.fromstring(response.content)\n",
    "\n",
    "# Extract company names using XPath\n",
    "companies = tree.xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"table-striped\", \" \" ))]//a/text()')\n",
    "\n",
    "# Save the list of companies to a CSV file\n",
    "pd.DataFrame(companies, columns=[\"Company\"]).to_csv(\"companies.csv\", index=False)\n",
    "\n",
    "# Read the list back into a DataFrame to confirm\n",
    "df = pd.read_csv(\"companies.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Company Names and Finding LinkedIn Pages\n",
    "Once we have a list of companies from our CSV file, our next objective is to search for these companies on LinkedIn. However, company names can often include designations like 'LLC', 'Inc.', etc., which might interfere with our search. So, the first step is to clean up these names.\n",
    "\n",
    "After that, we programmatically visit LinkedIn's company search page for each cleaned company name, scrape the LinkedIn URL of the company, and store these URLs for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_company_name(company_name):\n",
    "    # List of extra stuff we want to remove from the company name\n",
    "    extra_stuff = ['LLC', 'LP', 'Inc.', 'Co.', 'Corp.', 'Ltd.', 'LLP', 'PLC', 'AG', 'AB', 'BV', 'GmbH']\n",
    "    \n",
    "    # Remove extra stuff from the name\n",
    "    cleaned_name = ' '.join(word for word in company_name.split() if word not in extra_stuff)\n",
    "    \n",
    "    # Remove any commas\n",
    "    cleaned_name = cleaned_name.replace(',', '')\n",
    "    \n",
    "    # Replace spaces with URL-encoded spaces (%20) for the URL\n",
    "    cleaned_name = cleaned_name.replace(' ', '%20')\n",
    "    \n",
    "    return cleaned_name\n",
    "\n",
    "# List to hold company LinkedIn URLs\n",
    "company_links = []\n",
    "\n",
    "# Iterate through DataFrame rows\n",
    "for i, row in df.iterrows():\n",
    "    # Clean the company name\n",
    "    clean_name = clean_company_name(row['Company'])\n",
    "    \n",
    "    # Generate LinkedIn search URL for the cleaned company name\n",
    "    company_search_page = f\"https://www.linkedin.com/search/results/companies/?keywords={clean_name}\"\n",
    "    print(company_search_page)\n",
    "    \n",
    "    # Navigate to LinkedIn search page\n",
    "    driver.get(company_search_page)\n",
    "    sleep(2) # Wait 2 seconds for the page to load\n",
    "    \n",
    "    # Try to find the LinkedIn URL of the company\n",
    "    try:\n",
    "        # XPath for most common scenario\n",
    "        element = driver.find_element_by_xpath('/html/body/div[5]/div[3]/div[2]/div/div[1]/main/div/div/div[2]/div/ul/li[1]/div/div/div[2]/div[1]/div[1]/div/span/span/a')\n",
    "    except:\n",
    "        try:\n",
    "            # Backup XPath for less common scenario\n",
    "            element = driver.find_element_by_xpath('/html/body/div[5]/div[3]/div[2]/div/div[1]/main/div/div/div[3]/div/ul/li/div/div/div[2]/div[1]/div[1]/div/span/span/a')\n",
    "        except:\n",
    "            # If both fail, skip to next iteration\n",
    "            continue\n",
    "            \n",
    "    # Extract the LinkedIn URL\n",
    "    link = element.get_attribute(\"href\")\n",
    "    \n",
    "    # Append the URL to our list\n",
    "    company_links.append(link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting the LinkedIn URLs of the companies you are interested in, it's a good idea to save this data locally. This way, you can reuse the list without having to scrape LinkedIn again, saving both time and computational resources.\n",
    "\n",
    "In Python, one of the ways to save and load objects is by using the pickle module, which allows you to serialize (save) and deserialize (load) Python objects to and from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the list to a file\n",
    "filename = \"company_list.pkl\"\n",
    "# Uncomment the following lines to actually save the list\n",
    "# with open(filename, \"wb\") as file:\n",
    "#     pickle.dump(company_links, file)\n",
    "\n",
    "# Loading the list from the file\n",
    "with open(filename, \"rb\") as file:\n",
    "    loaded_list = pickle.load(file)\n",
    "\n",
    "# Printing the loaded list to confirm\n",
    "print(loaded_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Job Posting URLs from Company Pages\n",
    "After gathering LinkedIn URLs for companies you are interested in, the next step is to explore their job listings. We will navigate to each company's jobs page, scrape the URLs of individual job listings, and store them for later use.\n",
    "\n",
    "Here is how to collect job posting URLs from LinkedIn company pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold job LinkedIn URLs\n",
    "job_links = []\n",
    "\n",
    "# Iterate through each company's LinkedIn URL in the loaded list\n",
    "for company_url in loaded_list:\n",
    "    # Navigate to the 'jobs' section of the company's LinkedIn page\n",
    "    driver.get(company_url + \"jobs\")\n",
    "    \n",
    "    try:\n",
    "        # Try to find the link to all job listings for the company\n",
    "        element = driver.find_element_by_xpath('/html/body/div[5]/div[3]/div/div[2]/div/div[2]/main/div[2]/div/section/div/div/a')\n",
    "    except:\n",
    "        try:\n",
    "            # If the above fails, try another XPath pattern\n",
    "            element = driver.find_element_by_xpath('/html/body/div[4]/div[3]/div/div[2]/div/div[2]/main/div[2]/div/section/div/div/a')\n",
    "        except:\n",
    "            # If both XPaths fail, skip to the next company\n",
    "            continue\n",
    "\n",
    "    # Extract the URL of the page containing all job listings\n",
    "    link = element.get_attribute(\"href\")\n",
    "\n",
    "    # Navigate to the extracted link\n",
    "    driver.get(link)\n",
    "    sleep(1)  # Wait for a second to ensure the page loads\n",
    "    \n",
    "    # Find the ul element containing the job listings\n",
    "    ul_element = driver.find_element_by_xpath(\"/html/body/div[5]/div[3]/div[4]/div/div/main/div/div[1]/div/ul\")\n",
    "    \n",
    "    # Extract all the li elements (each represents a job listing) within the ul\n",
    "    li_elements = ul_element.find_elements_by_tag_name(\"li\")\n",
    "\n",
    "    # Iterate over each li element to extract job links\n",
    "    for element in li_elements:\n",
    "        link_element = element.find_elements_by_class_name(\"job-card-list__title\")\n",
    "        if link_element:\n",
    "            job_links.append(link_element[0].get_attribute(\"href\"))\n",
    "\n",
    "# Filename to store the list of job URLs\n",
    "filename = \"job_links.pkl\"\n",
    "\n",
    "# Saving the list to a file (uncomment when ready to save)\n",
    "# with open(filename, \"wb\") as file:\n",
    "#     pickle.dump(job_links, file)\n",
    "\n",
    "# Loading the list back from the saved file\n",
    "with open(filename, \"rb\") as file:\n",
    "    loaded_job_links = pickle.load(file)\n",
    "\n",
    "# Printing the loaded list to ensure it loaded correctly\n",
    "print(loaded_job_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Job Details\n",
    "We have previously managed to compile lists of both company and job URLs. The next vital step is to extract relevant details from the job postings themselves. This will include information like the job title, the qualifications needed, and the hiring manager’s name, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty list to hold all the scraped job data\n",
    "job_data = []\n",
    "\n",
    "# Iterate over each job link that we've previously loaded\n",
    "for job in loaded_list:\n",
    "    try:\n",
    "        # Navigate to the job URL\n",
    "        driver.get(job)\n",
    "\n",
    "        job_details = {}  # Initialize an empty dictionary to hold details for this job\n",
    "\n",
    "        # Scraping different fields for job details\n",
    "        try:\n",
    "            job_details['job_title'] = driver.find_element_by_class_name(\"jobs-unified-top-card__job-title\").text\n",
    "        except Exception:\n",
    "            print(\"Exception when fetching job title\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "        try:\n",
    "            job_details['job_metadata'] = driver.find_element_by_class_name(\"jobs-unified-top-card__primary-description\").text\n",
    "        except Exception:\n",
    "            print(\"Exception when fetching job metadata\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "        try:\n",
    "            job_details['hiring_team_name'] = driver.find_element_by_class_name(\"jobs-poster__name\").text\n",
    "        except Exception:\n",
    "            print(\"Exception when fetching hiring team name\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "        try:\n",
    "            test = driver.find_element_by_class_name(\"hirer-card__hirer-information\")\n",
    "            a_element = test.find_element_by_xpath(\".//a\")\n",
    "            job_details['hiring_team_link'] = a_element.get_attribute(\"href\")\n",
    "        except Exception:\n",
    "            print(\"Exception when fetching hiring team link\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "        company_metadata = []\n",
    "        try:\n",
    "            for i in driver.find_elements_by_class_name(\"jobs-unified-top-card__job-insight\"):\n",
    "                company_metadata.append(i.text)\n",
    "            job_details['company_metadata'] = company_metadata\n",
    "        except Exception:\n",
    "            print(\"Exception when fetching company metadata\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "        try:\n",
    "            job_details['description'] = driver.find_element_by_class_name(\"jobs-description-content__text\").text\n",
    "        except Exception:\n",
    "            print(\"Exception when fetching job description\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "        # Append the collected job details to the master list\n",
    "        job_data.append(job_details)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Exception when processing job: \", job)\n",
    "        print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of job details dictionaries into a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Add a column for the LinkedIn job links\n",
    "df['link'] = loaded_list  # We already loaded these job links in a previous step\n",
    "\n",
    "# Preview the DataFrame to check the data\n",
    "df.head()\n",
    "\n",
    "# OPTIONAL: If you've saved the data to a CSV and you're reading it back in\n",
    "# df = pd.read_csv(\"final_output.csv\", index_col=0)\n",
    "\n",
    "# Clean up the 'description' column to replace newline characters with a space\n",
    "df['description'] = df['description'].str.replace(r'\\r?\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Hugging Face's Transformers for Text Analysis\n",
    "In recent years, Natural Language Processing (NLP) has made significant strides, and much of that has been possible due to the groundbreaking work in transformers. Hugging Face, an AI research organization, has made it easier for developers to access these high-end machine learning models for various NLP tasks through their Transformers library. This open-source library offers a collection of pre-trained models and pipelines to perform tasks such as text summarization, text generation, translation, and more.\n",
    "\n",
    "Hugging Face's Transformers is particularly helpful in our context of job application automation because it allows us to better understand and interact with job descriptions and requirements. With pre-trained models, we can automatically summarize lengthy job descriptions to quickly grasp essential details or even generate personalized cover letters or application snippets based on job postings.\n",
    "\n",
    "### Summarization with Hugging Face\n",
    "The summarization task is invaluable when you're dealing with extensive job descriptions that require a lot of time to read through. With just a few lines of code, you can summarize these descriptions, capturing the most important points. We'll be using \"sshleifer/distilbart-cnn-12-6,\" a model fine-tuned for summarization tasks.\n",
    "\n",
    "### Text Generation with Hugging Face\n",
    "Beyond summarization, text generation can offer more creative ways to interact with job data. For example, based on the skills required in the job descriptions, you could generate a paragraph highlighting your matching skills and experiences. For this, we'll use the \"meta-llama/Llama-2-7b-chat-hf\" model, which specializes in generating human-like conversational text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pipeline from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "get_completion = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "# Initialize the text-generation pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the summarization pipeline set up, the next logical step is to apply this capability to our existing job descriptions. Summarizing each job description can make it easier to quickly understand the essence of each job posting.\n",
    "\n",
    "### The summarize Function\n",
    "Let's now define a function called summarize, which will take an input text and return its summarized version. We'll also include exception handling to make sure that the function fails gracefully if it encounters any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input):\n",
    "    try:\n",
    "        # Using the get_completion pipeline to summarize the input\n",
    "        output = get_completion(input)\n",
    "        return output[0]['summary_text']\n",
    "    except ValueError:\n",
    "        # Handle the ValueError here if it occurs\n",
    "        pass\n",
    "\n",
    "# Applying the summarize function to the 'description' column in our DataFrame\n",
    "df['description_summary'] = df['description'].apply(summarize)\n",
    "\n",
    "# Saving the DataFrame with the summarized descriptions\n",
    "df.to_csv(\"summarized_description.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Generating Cover Letters with Text Generation\n",
    "After summarizing the job descriptions, a key element that remains is creating a cover letter tailored to each job. Writing a unique and compelling cover letter for every job application is a time-consuming process. Wouldn't it be wonderful if we could automate this as well?\n",
    "\n",
    "In this section, we will define a function that will utilize Hugging Face's text generation capabilities to produce a personalized cover letter for each job posting.\n",
    "\n",
    "### The generate_cover_letter Function\n",
    "We will define a function called generate_cover_letter that will take in the row of the DataFrame, which contains summarized job descriptions, job titles, and other metadata. This function will generate a cover letter specific to each job description.\n",
    "\n",
    "Here's the function code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cover_letter(row):\n",
    "    # Create a prompt incorporating company name, job title, and description summary\n",
    "    prompt = f\"Dear Hiring Team at {row['Company']},\\n\\nI am excited to apply for the {row['job_title']} position. I was particularly drawn to this role because {row['description_summary']}. Can you please generate a cover letter for me?\"\n",
    "\n",
    "    try:\n",
    "        # Using the 'pipe' pipeline for text generation based on the prompt\n",
    "        generated_text = pipe(prompt, max_length=500, do_sample=True, top_k=50)[0]['generated_text']\n",
    "        \n",
    "        # Extracting the generated cover letter from the generated_text\n",
    "        start = generated_text.find(\"Dear Hiring Team\")\n",
    "        if start != -1:\n",
    "            generated_cover_letter = generated_text[start:]\n",
    "            return generated_cover_letter\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Applying the generate_cover_letter function to each row in the DataFrame\n",
    "df['generated_cover_letter'] = df.apply(generate_cover_letter, axis=1)\n",
    "\n",
    "# Saving the DataFrame with generated cover letters\n",
    "df.to_csv(\"with_generated_cover_letters.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the generation part with ollama instead\n",
    "\n",
    "Since I wrote this article, ollama came out which if I'm being honest is porbably the best tool to run llm models as it supports calling the function straight from python or terminal.\n",
    "\n",
    "Let's start by re reading the [last csv](summarized_description.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"summarized_description.csv\", index_col=0)\n",
    "\n",
    "\n",
    "def compose_email(hiring_team_name, job_title, job_metadata, hiring_team_link, company_metadata,description, link, description_summary):\n",
    "    \"\"\"\n",
    "    Generates a personalized email for a job application using provided details.\n",
    "\n",
    "    Parameters:\n",
    "    - hiring_team_name: The name of the person in the hiring team.\n",
    "    - job_title: The title of the job.\n",
    "    - job_metadata: Additional details about the job.\n",
    "    - hiring_team_link: A link to learn more about the hiring team or the person.\n",
    "    - company_metadata: Information about the company.\n",
    "    - link: A link to the job posting.\n",
    "    - description_summary: A summary of the job description, emphasizing key responsibilities and qualifications.\n",
    "\n",
    "    Returns:\n",
    "    A string containing the personalized email.\n",
    "    \"\"\"\n",
    "    email_body = f\"\"\"\n",
    "Dear {hiring_team_name},\n",
    "\n",
    "I am reaching out to express my genuine interest in the {job_title} position as advertised on {link}. With a solid background in {job_metadata}, I am enthusiastic about the opportunity to contribute to {company_metadata}, a company I greatly admire for its [insert reason based on company_metadata].\n",
    "\n",
    "The role's focus on {description_summary} aligns perfectly with my professional skills and personal interests. I am confident that my experience in [mention relevant experience] makes me a strong candidate for this position. I am particularly excited about the chance to [mention a specific project or responsibility mentioned in the job description].\n",
    "\n",
    "Here’s why I believe I am a good fit for the role:\n",
    "- [Briefly highlight key qualifications and experiences relevant to the job description_summary]\n",
    "\n",
    "I am very much looking forward to the possibility of discussing this exciting opportunity with you further. Please let me know if you need any more information or if there's a convenient time for us to discuss how I can contribute to your team.\n",
    "\n",
    "Thank you for considering my application.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "\"\"\"\n",
    "\n",
    "    return email_body.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title                                            Investment Engineer\n",
       "job_metadata           Bridgewater Associates · Westport, CT (On-site...\n",
       "hiring_team_name                                         Jason Koulouras\n",
       "hiring_team_link              https://www.linkedin.com/in/jasonkoulouras\n",
       "description            About the job About Bridgewater  Bridgewater A...\n",
       "company_metadata                                                     NaN\n",
       "link                   https://www.linkedin.com/jobs/view/3659222868/...\n",
       "description_summary     Investment Engineer's mission is to understan...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bring back first row for testing\n",
    "row = df.iloc[0]\n",
    "row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = compose_email(**row)\n",
    "message\n",
    "\n",
    "model_input = {\n",
    "    'role': 'system',  # or 'user', depending on the context\n",
    "    'content': message\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Jason Koulouras,\n",
      "\n",
      "I am writing to express my strong interest in the Investment Engineer position at Bridgewater Associates. I was drawn to this opportunity due to the company's reputation as a leader in the financial industry and its commitment to innovation and excellence. As an experienced investment professional with a solid background in [relevant field], I am confident that my skills and experience align perfectly with the requirements of this role.\n",
      "\n",
      "From my research on Bridgewater Associates, I understand that the Investment Engineer's primary responsibility is to design and build the algorithms used to generate the company's views on markets and economies, as well as translate those views into portfolios and trade them. As someone who shares the company's passion for understanding how these systems work and using that knowledge to inform investment decisions, I am excited about the opportunity to contribute to this mission.\n",
      "\n",
      "In my current role at [current company], I have gained valuable experience in [relevant skills or experiences]. These skills, combined with my ability to work well in a team and communicate complex ideas effectively, make me a strong candidate for this position. I am particularly enthusiastic about the opportunity to work on [specific project or responsibility mentioned in the job description] and contribute my expertise to help drive Bridgewater's success.\n",
      "\n",
      "Thank you for considering my application. I would be thrilled to discuss how I can contribute to your team and help drive the company's continued growth and innovation. Please let me know if there is any additional information you need or if you would like to schedule a time to speak further.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='llama2', messages=[model_input])\n",
    "print(response['message']['content'])\n",
    "\n",
    "## Create a class or a function  to loop over the dataframe and send emails to the hiring team if you would like to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "This is not perfect, and you should not automate this application part. The whole idea of this project is to automate the data collection part of finding your perfect job; the application should always have your personal touch, as it will be the only thing that will stand out from the 1000 resumes that are being flooded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You can still do more with the LLM part, filter out jobs you don't want, or get involved in creating more specialized emails. The sky (or your processing power) is the limit.\n",
    "\n",
    "\n",
    "Automation in job applications, mainly through data scraping and machine learning, can significantly streamline the hiring process for job seekers and recruiters. This tutorial demonstrates how one can automatically collect job postings from LinkedIn and then use machine learning models, specifically from Hugging Face's Transformers library, to generate personalized cover letters.\n",
    "\n",
    "We've walked through how to set up a web scraper with Selenium, fetch relevant job details, and utilize the NLP power of Hugging Face to generate content dynamically.\n",
    "\n",
    "However, I would like to point out that the applicant should review and fine-tune the generated cover letters to ensure they align with the individual's unique skills, experience, and aspirations. Automation tools can accelerate the process but only partially replace the human touch.\n",
    "\n",
    "Lastly, please ensure you comply with the terms and conditions or guidelines set by any third-party services you use. This tutorial is strictly educational and should not be considered an encouragement to violate such terms.\n",
    "\n",
    "Happy job hunting, and may your automation efforts land your dream job!\n",
    "\n",
    "That wraps up our article. I hope this tutorial is informative and useful. Thank you for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
